################################################################
# Config from TensorFlow's .bazelrc.                           #
#                                                              #
# This is copied directly, with some irrelevant parts removed. #
################################################################

# Default build options. These are applied first and unconditionally.

# For projects which use TensorFlow as part of a Bazel build process, putting
# nothing in a bazelrc will default to a monolithic build. The following line
# opts in to modular op registration support by default.
build --define framework_shared_object=true
build --define tsl_protobuf_header_only=true

build --define=use_fast_cpp_protos=true
build --define=allow_oversize_protos=true

build --spawn_strategy=standalone
build -c opt

# Make Bazel print out all options from rc files.
build --announce_rc

build --define=grpc_no_ares=true

build --noincompatible_remove_legacy_whole_archive
build --features=-force_no_whole_archive

build --enable_platform_specific_config

# Enable XLA support by default.
build --define=with_xla_support=true

build --config=short_logs

build --config=v2

# Disable AWS/HDFS support by default
build --define=no_aws_support=true
build --define=no_hdfs_support=true

# TF now has `cc_shared_library` targets, so it needs the experimental flag
# TODO(rostam): Remove when `cc_shared_library` is enabled by default
build --experimental_cc_shared_library

# cc_shared_library ensures no library is linked statically more than once.
build --experimental_link_static_libraries_once=false

# Prevent regressions on those two incompatible changes
# TODO: remove those flags when they are flipped in the default Bazel version TF uses.
build --incompatible_enforce_config_setting_visibility

# Default options should come above this line.

# Android configs. Bazel needs to have --cpu and --fat_apk_cpu both set to the
# target CPU to build transient dependencies correctly. See
# https://docs.bazel.build/versions/master/user-manual.html#flag--fat_apk_cpu
build:android --crosstool_top=//external:android/crosstool
build:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
build:android_arm --config=android
build:android_arm --cpu=armeabi-v7a
build:android_arm --fat_apk_cpu=armeabi-v7a
build:android_arm64 --config=android
build:android_arm64 --cpu=arm64-v8a
build:android_arm64 --fat_apk_cpu=arm64-v8a
build:android_x86 --config=android
build:android_x86 --cpu=x86
build:android_x86 --fat_apk_cpu=x86
build:android_x86_64 --config=android
build:android_x86_64 --cpu=x86_64
build:android_x86_64 --fat_apk_cpu=x86_64

# Build everything statically for Android since all static libs are later
# bundled together into a single .so for deployment.
build:android --dynamic_mode=off

# Sets the default Apple platform to macOS.
build:macos --apple_platform_type=macos

# gRPC on MacOS requires this #define
build:macos --copt=-DGRPC_BAZEL_BUILD

# Avoid hitting command line argument limit
build:macos --features=archive_param_file

# Settings for MacOS on ARM CPUs.
build:macos_arm64 --cpu=darwin_arm64
build:macos_arm64 --macos_minimum_os=11.0

# Config to use a mostly-static build and disable modular op registration
# support (this will revert to loading TensorFlow with RTLD_GLOBAL in Python).
# By default, TensorFlow will build with a dependence on
# //tensorflow:libtensorflow_framework.so.
build:monolithic --define framework_shared_object=false
build:monolithic --define tsl_protobuf_header_only=false
build:monolithic --experimental_link_static_libraries_once=false  # b/229868128

# Please note that MKL on MacOS is still not supported.
# If you would like to use a local MKL instead of downloading, please set the
# environment variable "TF_MKL_ROOT" every time before build.
build:mkl --define=build_with_mkl=true --define=enable_mkl=true
build:mkl --define=tensorflow_mkldnn_contraction_kernel=0
build:mkl --define=build_with_openmp=true
build:mkl -c opt

# config to build OneDNN backend with a user specified threadpool.
build:mkl_threadpool --define=build_with_mkl=true --define=enable_mkl=true
build:mkl_threadpool --define=tensorflow_mkldnn_contraction_kernel=0
build:mkl_threadpool --define=build_with_mkl_opensource=true
build:mkl_threadpool -c opt

# Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).
build:mkl_aarch64 --define=build_with_mkl_aarch64=true
build:mkl_aarch64 --define=build_with_openmp=true
build:mkl_aarch64 --define=build_with_acl=true
build:mkl_aarch64 -c opt

# Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).
# with Eigen threadpool support
build:mkl_aarch64_threadpool --define=build_with_mkl_aarch64=true
build:mkl_aarch64_threadpool -c opt

# CUDA: This config refers to building CUDA op kernels with nvcc.
build:cuda --repo_env TF_NEED_CUDA=1
build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain
build:cuda --@local_config_cuda//:enable_cuda

# CUDA: This config refers to building CUDA op kernels with clang.
build:cuda_clang --config=cuda
# Enable TensorRT optimizations https://developer.nvidia.com/tensorrt
build:cuda_clang --config=tensorrt
build:cuda_clang --action_env=TF_CUDA_CLANG="1"
build:cuda_clang --@local_config_cuda//:cuda_compiler=clang
# Select supported compute capabilities (supported graphics cards).
# This is the same as the official TensorFlow builds.
# See https://developer.nvidia.com/cuda-gpus#compute
# `compute_XY` enables PTX embedding in addition to SASS. PTX
# is forward compatible beyond the current compute capability major
# release while SASS is only forward compatible inside the current
# major release. Example: sm_80 kernels can run on sm_89 GPUs but
# not on sm_90 GPUs. compute_80 kernels though can also run on sm_90 GPUs.
build:cuda_clang --repo_env=TF_CUDA_COMPUTE_CAPABILITIES="sm_50,sm_60,sm_70,sm_80,compute_90"

# Set up compilation CUDA version and paths and use the CUDA Clang toolchain.
build:cuda_clang_official --config=cuda_clang
build:cuda_clang_official --action_env=TF_CUDA_VERSION="12"
build:cuda_clang_official --action_env=TF_CUDNN_VERSION="8"
build:cuda_clang_official --action_env=CUDA_TOOLKIT_PATH="/usr/local/cuda-12.3"
build:cuda_clang_official --action_env=GCC_HOST_COMPILER_PATH="/dt9/usr/bin/gcc"
build:cuda_clang_official --action_env=CLANG_CUDA_COMPILER_PATH="/usr/lib/llvm-17/bin/clang"
build:cuda_clang_official --action_env=LD_LIBRARY_PATH="/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64"
build:cuda_clang_official --crosstool_top="@sigbuild-r2.16-clang_config_cuda//crosstool:toolchain"

# Build with nvcc for CUDA and clang for host
build:nvcc_clang --config=cuda
# Unfortunately, cuda_configure.bzl demands this for using nvcc + clang
build:nvcc_clang --action_env=TF_CUDA_CLANG="1"
build:nvcc_clang --action_env=TF_NVCC_CLANG="1"
build:nvcc_clang --@local_config_cuda//:cuda_compiler=nvcc


# Debug config
build:dbg -c dbg
# Only include debug info for files under tensorflow/, excluding kernels, to
# reduce the size of the debug info in the binary. This is because if the debug
# sections in the ELF binary are too large, errors can occur. See
# https://github.com/tensorflow/tensorflow/issues/48919.
# Users can still include debug info for a specific kernel, e.g. with:
#     --config=dbg --per_file_copt=+tensorflow/core/kernels/identity_op.*@-g
# Since this .bazelrc file is synced between the tensorflow/tensorflow repo and
# the openxla/xla repo, also include debug info for files under xla/.
build:dbg --per_file_copt=+.*,-tensorflow.*,-xla.*@-g0
build:dbg --per_file_copt=+tensorflow/core/kernels.*@-g0
# for now, disable arm_neon. see: https://github.com/tensorflow/tensorflow/issues/33360
build:dbg --cxxopt -DTF_LITE_DISABLE_X86_NEON
# AWS SDK must be compiled in release mode. see: https://github.com/tensorflow/tensorflow/issues/37498
build:dbg --copt -DDEBUG_BUILD

# Config to build TF TPU
build:tpu --define=with_tpu_support=true
build:tpu --define=framework_shared_object=true
build:tpu --copt=-DLIBTPU_ON_GCE
build:tpu --define=enable_mlir_bridge=true

build:tensorrt --repo_env TF_NEED_TENSORRT=1

build:rocm --crosstool_top=@local_config_rocm//crosstool:toolchain
build:rocm --define=using_rocm_hipcc=true
build:rocm --define=tensorflow_mkldnn_contraction_kernel=0
build:rocm --repo_env TF_NEED_ROCM=1

# Options to disable default on features
build:noaws --define=no_aws_support=true
build:nogcp --define=no_gcp_support=true
build:nohdfs --define=no_hdfs_support=true
build:nonccl --define=no_nccl_support=true

# Modular TF build options
build:dynamic_kernels --define=dynamic_loaded_kernels=true
build:dynamic_kernels --copt=-DAUTOLOAD_DYNAMIC_KERNELS

# Don't trigger --config=<host platform> when cross-compiling.
build:android --noenable_platform_specific_config
build:ios --noenable_platform_specific_config

# Suppress all C++ compiler warnings, otherwise build logs become 10s of MBs.
build:android --copt=-w
build:ios --copt=-w
build:linux --host_copt=-w
build:macos --copt=-w
build:windows --copt=/W0
build:windows --host_copt=/W0

# Suppress most C++ compiler warnings to reduce log size but allow
# for specific warnings to still be present.
build:linux --copt="-Wno-all"
build:linux --copt="-Wno-extra"
build:linux --copt="-Wno-deprecated"
build:linux --copt="-Wno-deprecated-declarations"
build:linux --copt="-Wno-ignored-attributes"
build:linux --copt="-Wno-array-bounds"

# Add unused-result as an error on Linux.
build:linux --copt="-Wunused-result"
build:linux --copt="-Werror=unused-result"
# Add switch as an error on Linux.
build:linux --copt="-Wswitch"
build:linux --copt="-Werror=switch"
# Required for building with clang
build:linux --copt="-Wno-error=unused-but-set-variable"

# Linux ARM64 specific options
build:linux_arm64 --copt="-mtune=generic" --copt="-march=armv8-a" --copt="-O3"


# Suppress C++ compiler warnings, otherwise build logs become 10s of MBs.
build:android --copt=-w
build:ios --copt=-w
build:linux --copt=-w
build:linux --host_copt=-w
build:macos --copt=-w
build:windows --copt=/W0
build:windows --host_copt=/W0

# On Windows, `__cplusplus` is wrongly defined without this switch
# See https://devblogs.microsoft.com/cppblog/msvc-now-correctly-reports-__cplusplus/
build:windows --copt=/Zc:__cplusplus
build:windows --host_copt=/Zc:__cplusplus

# Tensorflow uses M_* math constants that only get defined by MSVC headers if
# _USE_MATH_DEFINES is defined.
build:windows --copt=/D_USE_MATH_DEFINES
build:windows --host_copt=/D_USE_MATH_DEFINES

# Windows has a relatively short command line limit, which TF has begun to hit.
# See https://docs.bazel.build/versions/main/windows.html
build:windows --features=compiler_param_file
build:windows --features=archive_param_file

# Speed Windows compile times. Available in VS 16.4 (we are on 16.11). See
# https://groups.google.com/a/tensorflow.org/d/topic/build/SsW98Eo7l3o/discussion
build:windows --copt=/d2ReducedOptimizeHugeFunctions
build:windows --host_copt=/d2ReducedOptimizeHugeFunctions

# Enable the runfiles symlink tree on Windows. This makes it possible to build
# the pip package on Windows without an intermediate data-file archive, as the
# build_pip_package script in its current form (as of Aug 2023) uses the
# runfiles symlink tree to decide what to put into the Python wheel.
startup --windows_enable_symlinks
build:windows --enable_runfiles

# Default paths for TF_SYSTEM_LIBS
build:linux --define=PREFIX=/usr
build:linux --define=LIBDIR=$(PREFIX)/lib
build:linux --define=INCLUDEDIR=$(PREFIX)/include
build:linux --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include
build:macos --define=PREFIX=/usr
build:macos --define=LIBDIR=$(PREFIX)/lib
build:macos --define=INCLUDEDIR=$(PREFIX)/include
build:macos --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include
# TF_SYSTEM_LIBS do not work on windows.

# By default, build TF in C++ 17 mode.
build:android --cxxopt=-std=c++17
build:android --host_cxxopt=-std=c++17
build:linux --cxxopt=-std=c++17
build:linux --host_cxxopt=-std=c++17
build:macos --cxxopt=-std=c++17
build:macos --host_cxxopt=-std=c++17
build:windows --cxxopt=/std:c++17
build:windows --host_cxxopt=/std:c++17

# On windows, we still link everything into a single DLL.
build:windows --config=monolithic

# On linux, we dynamically link small amount of kernels
build:linux --config=dynamic_kernels

# Make sure to include as little of windows.h as possible
build:windows --copt=-DWIN32_LEAN_AND_MEAN
build:windows --host_copt=-DWIN32_LEAN_AND_MEAN
build:windows --copt=-DNOGDI
build:windows --host_copt=-DNOGDI

# MSVC (Windows): Standards-conformant preprocessor mode
# See https://docs.microsoft.com/en-us/cpp/preprocessor/preprocessor-experimental-overview
build:windows --copt=/Zc:preprocessor
build:windows --host_copt=/Zc:preprocessor

# Misc build options we need for windows.
build:windows --linkopt=/DEBUG
build:windows --host_linkopt=/DEBUG
build:windows --linkopt=/OPT:REF
build:windows --host_linkopt=/OPT:REF
build:windows --linkopt=/OPT:ICF
build:windows --host_linkopt=/OPT:ICF

# Verbose failure logs when something goes wrong
build:windows --verbose_failures

# Work around potential issues with large command lines on windows.
# See: https://github.com/bazelbuild/bazel/issues/5163
build:windows --features=compiler_param_file

# Do not risk cache corruption. See:
# https://github.com/bazelbuild/bazel/issues/3360
build:linux --experimental_guard_against_concurrent_changes

# Configure short or long logs
build:short_logs --output_filter=DONT_MATCH_ANYTHING
build:verbose_logs --output_filter=

# Instruction set optimizations
# TODO(gunan): Create a feature in toolchains for avx/avx2 to
#   avoid having to define linux/win separately.
build:avx_linux --copt=-mavx
build:avx_linux --host_copt=-mavx
build:avx_win --copt=/arch:AVX

# Use Clang-cl compiler on Windows
build:win_clang --copt=/clang:-Weverything
build:win_clang --extra_toolchains=@local_config_cc//:cc-toolchain-x64_windows-clang-cl
build:win_clang --extra_execution_platforms=//tensorflow/tools/toolchains/win:x64_windows-clang-cl
build:win_clang --host_platform=//tensorflow/tools/toolchains/win:x64_windows-clang-cl
build:win_clang --compiler=clang-cl
build:win_clang --linkopt=/FORCE:MULTIPLE
build:win_clang --host_linkopt=/FORCE:MULTIPLE
test:win_clang --linkopt=/FORCE:MULTIPLE
test:win_clang --host_linkopt=/FORCE:MULTIPLE

# Options to build TensorFlow 1.x or 2.x.
# TODO(kanglan): Change v2's define to default behavior
build:v2 --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1

# Disable XLA on mobile.
build:xla     --define=with_xla_support=true # TODO: remove, it's on by default.
build:android --define=with_xla_support=false
build:ios     --define=with_xla_support=false

# Flag to enable remote config
common --experimental_repo_remote_exec

# TFLite build configs for generic embedded Linux
build:elinux --crosstool_top=@local_config_embedded_arm//:toolchain
build:elinux --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
build:elinux_aarch64 --config=elinux
build:elinux_aarch64 --cpu=aarch64
build:elinux_armhf --config=elinux
build:elinux_armhf --cpu=armhf
build:elinux_armhf --copt -mfp16-format=ieee

# Config-specific options should come above this line.

# Load rc file written by ./configure.
try-import %workspace%/.tf_configure.bazelrc
try-import %workspace%/xla_configure.bazelrc

# Here are bazelrc configs for release builds
# Build TensorFlow v2.
test:release_base --test_size_filters=small,medium
test:release_base --flaky_test_attempts=3

# Target the AVX instruction set
build:release_linux_base --config=avx_linux

# Disable clang extension that rejects type definitions within offsetof.
# This was added in clang-16 by https://reviews.llvm.org/D133574.
# Can be removed once upb is updated, since a type definition is used within
# offset of in the current version of ubp.
# See https://github.com/protocolbuffers/upb/blob/9effcbcb27f0a665f9f345030188c0b291e32482/upb/upb.c#L183.
build:release_linux_base --copt=-Wno-gnu-offsetof-extensions
build:release_linux_base --copt=-Wno-error=array-parameter
build:release_linux_base --copt=-Wno-error=unused-command-line-argument
# Set lld as the linker.
build:release_linux_base --linkopt="-fuse-ld=lld"
build:release_linux_base --linkopt="-lm"

# We have some invalid linker scripts in the build,
# so we need to disable this check
build:release_linux_base --linkopt=-Wl,--undefined-version

# Container environment settings below this point.
# Use Python 3.X as installed in container image
build:release_linux_base --action_env PYTHON_BIN_PATH="/usr/bin/python3"
build:release_linux_base --action_env PYTHON_LIB_PATH="/usr/lib/tf_python"
build:release_linux_base --python_path="/usr/bin/python3"
# Set Clang as compiler. Use the actual path to clang installed in container.
build:release_cpu_linux_base --repo_env=CC="/usr/lib/llvm-17/bin/clang"
build:release_cpu_linux_base --repo_env=BAZEL_COMPILER="/usr/lib/llvm-17/bin/clang"
# Test-related settings below this point.
test:release_linux_base --build_tests_only --keep_going --test_output=errors --verbose_failures=true
test:release_linux_base --local_test_jobs=HOST_CPUS
test:release_linux_base --test_env=LD_LIBRARY_PATH
# Give only the list of failed tests at the end of the log
test:release_linux_base --test_summary=short

# Use the Clang toolchain to compile
build:release_cpu_linux --config=release_linux_base
build:release_cpu_linux --crosstool_top="@sigbuild-r2.16-clang_config_cuda//crosstool:toolchain"

build:release_gpu_linux --config=release_cpu_linux
# Set up compilation CUDA version and paths and use the CUDA Clang toolchain.
# Note that linux cpu and cuda builds share the same toolchain now.
build:release_gpu_linux --config=cuda_clang_official
test:release_gpu_linux --test_env=LD_LIBRARY_PATH="/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64"
# Local test jobs has to be 4 because parallel_gpu_execute is fragile, I think
test:release_gpu_linux --test_timeout=300,450,1200,3600 --local_test_jobs=4 --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute

build:release_arm64_linux --config=release_linux_base
build:release_arm64_linux --config=linux_arm64
build:release_arm64_linux --crosstool_top="@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain"
build:release_arm64_linux --config=mkl_aarch64_threadpool
build:release_arm64_linux --copt=-flax-vector-conversions
test:release_arm64_linux --flaky_test_attempts=3

# The old gcc linux build options are preserved in the unsupported_*_linux
# configs. If your project fails to build with Clang, you can use these
# unsupported flags to replace the release flags in your build command.
# However, please note that the old toolchain is no longer officially supported
# by TensorFlow and the unsupported configs will be removed soon b/299962977. We
# strongly recommend that you migrate to Clang as your compiler for TensorFlow
# Linux builds. Instructions are available in the official documentation:
# https://www.tensorflow.org/install/source#install_clang_recommended_linux_only
# Another good option is to use our Docker containers to build and test TF:
# https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/tf_sig_build_dockerfiles.
build:unsupported_cpu_linux --config=avx_linux
build:unsupported_cpu_linux --crosstool_top="@ubuntu20.04-gcc9_manylinux2014-cuda11.2-cudnn8.1-tensorrt7.2_config_cuda//crosstool:toolchain"
test:unsupported_cpu_linux --test_env=LD_LIBRARY_PATH
test:unsupported_cpu_linux --config=release_base

build:unsupported_gpu_linux --config=cuda
build:unsupported_gpu_linux --config=unsupported_cpu_linux
build:unsupported_gpu_linux --action_env=TF_CUDA_VERSION="11"
build:unsupported_gpu_linux --action_env=TF_CUDNN_VERSION="8"
build:unsupported_gpu_linux --repo_env=TF_CUDA_COMPUTE_CAPABILITIES="sm_35,sm_50,sm_60,sm_70,sm_75,compute_80"
build:unsupported_gpu_linux --config=tensorrt
build:unsupported_gpu_linux --action_env=CUDA_TOOLKIT_PATH="/usr/local/cuda-11.2"
build:unsupported_gpu_linux --action_env=LD_LIBRARY_PATH="/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.1/lib64:/usr/local/tensorrt/lib"
build:unsupported_gpu_linux --action_env=GCC_HOST_COMPILER_PATH="/dt9/usr/bin/gcc"
build:unsupported_gpu_linux --crosstool_top=@ubuntu20.04-gcc9_manylinux2014-cuda11.2-cudnn8.1-tensorrt7.2_config_cuda//crosstool:toolchain

build:release_cpu_macos --config=avx_linux
test:release_cpu_macos --config=release_base

# Base build configs for macOS
build:release_macos_base --action_env  DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer
build:release_macos_base --define=no_nccl_support=true --output_filter=^$

# Build configs for macOS x86
build:release_macos_x86 --config=release_macos_base
# Build with the AVX instruction set when on macOS x86
build:release_macos_x86 --config=avx_linux
build:release_macos_x86 --cpu=darwin
# Target Catalina as the minimum compatible OS version
build:release_macos_x86 --macos_minimum_os=10.15
build:release_macos_x86 --action_env MACOSX_DEPLOYMENT_TARGET=10.15

# Build configs for macOS Arm64
build:release_macos_arm64 --config=release_macos_base
build:release_macos_arm64 --cpu=darwin_arm64
build:release_macos_arm64 --define=tensorflow_mkldnn_contraction_kernel=0
# Target Moneterey as the minimum compatible OS version
build:release_macos_arm64 --macos_minimum_os=12.0
build:release_macos_arm64 --action_env MACOSX_DEPLOYMENT_TARGET=12.0

# Base test configs for macOS
test:release_macos_base --verbose_failures=true --local_test_jobs=HOST_CPUS
test:release_macos_base --test_timeout=300,450,1200,3600 --test_output=errors
test:release_macos_base --build_tests_only --keep_going
test:release_macos_base --flaky_test_attempts=3

# Test configs for macOS x86
test:release_macos_x86 --config=release_macos_base

# Test configs for macOS Arm64
test:release_macos_arm64 --config=release_macos_base

# TODO(kanglan): Update windows configs after b/289091160 is fixed
build:release_cpu_windows --config=avx_win
build:release_cpu_windows --define=no_tensorflow_py_deps=true
test:release_cpu_windows --config=release_base

# Exclude TFRT integration for anything but Linux.
build:android --config=no_tfrt
build:macos   --config=no_tfrt
build:windows --config=no_tfrt
build:rocm --config=no_tfrt
build:no_tfrt --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils

# START CROSS-COMPILE CONFIGS
# Set execution platform to Linux x86
# Note: Lot of the "host_" flags such as "host_cpu" and "host_crosstool_top"
# flags seem to be actually used to specify the execution platform details. It
# seems it is this way because these flags are old and predate the distinction
# between host and execution platform.
build:cross_compile_base --host_cpu=k8
build:cross_compile_base --host_crosstool_top=//tensorflow/tools/toolchains/cross_compile/cc:cross_compile_toolchain_suite
build:cross_compile_base --extra_execution_platforms=//tensorflow/tools/toolchains/cross_compile/config:linux_x86_64

build:rbe_cross_compile_base --config=rbe_base
build:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/instances/default_instance

# Test-related settings below this point
# We cannot run cross-compiled tests on the remote Linux x86 VMs so we need to
# force all tests to run locally on the Aarch64 host.
test:rbe_cross_compile_base --strategy=TestRunner=local --build_tests_only
test:rbe_cross_compile_base --verbose_failures=true --local_test_jobs=HOST_CPUS --test_output=errors

# START LINUX AARCH64 CROSS-COMPILE CONFIGS
build:cross_compile_linux_arm64 --config=cross_compile_base

# Set the target CPU to Aarch64
build:cross_compile_linux_arm64 --platforms=//tensorflow/tools/toolchains/cross_compile/config:linux_aarch64
build:cross_compile_linux_arm64 --cpu=aarch64
build:cross_compile_linux_arm64 --crosstool_top=//tensorflow/tools/toolchains/cross_compile/cc:cross_compile_toolchain_suite

# RBE cross-compile configs for Linux Aarch64
build:rbe_cross_compile_linux_arm64 --config=cross_compile_linux_arm64
build:rbe_cross_compile_linux_arm64 --config=rbe_cross_compile_base
test:rbe_cross_compile_linux_arm64 --config=rbe_cross_compile_base
# END LINUX AARCH64 CROSS-COMPILE CONFIGS

# START MACOS CROSS-COMPILE CONFIGS
build:cross_compile_macos_x86 --config=cross_compile_base
build:cross_compile_macos_x86 --config=nonccl
# Target Catalina (10.15) as the minimum supported OS
build:cross_compile_macos_x86 --action_env  MACOSX_DEPLOYMENT_TARGET=10.15

# Set the target CPU to Darwin x86
build:cross_compile_macos_x86 --platforms=//tensorflow/tools/toolchains/cross_compile/config:darwin_x86_64
build:cross_compile_macos_x86 --cpu=darwin
build:cross_compile_macos_x86 --crosstool_top=//tensorflow/tools/toolchains/cross_compile/cc:cross_compile_toolchain_suite
# When RBE cross-compiling for macOS, we need to explicitly register the
# toolchain. Otherwise, oddly, RBE complains that a "docker container must be
# specified".
build:cross_compile_macos_x86 --extra_toolchains=//tensorflow/tools/toolchains/cross_compile/config:macos-x86-cross-compile-cc-toolchain
# Map --platforms=darwin_x86_64 to --cpu=darwin and vice-versa to make selects()
# and transistions that use these flags work.
build:cross_compile_macos_x86 --platform_mappings=tensorflow/tools/toolchains/cross_compile/config/platform_mappings

# RBE cross-compile configs for Darwin x86
build:rbe_cross_compile_macos_x86 --config=cross_compile_macos_x86
build:rbe_cross_compile_macos_x86 --config=rbe_cross_compile_base
test:rbe_cross_compile_macos_x86 --config=rbe_cross_compile_base
# Increase the test timeout as tests often take longer on mac.
test:rbe_cross_compile_macos_x86 --test_timeout=300,450,1200,3600
# Limit jobs to 100 to avoid running into "out of memory" issues (b/316266643)
build:rbe_cross_compile_macos_x86 --jobs=100
test:rbe_cross_compile_macos_x86 --jobs=100
# END MACOS CROSS-COMPILE CONFIGS
# END CROSS-COMPILE CONFIGS
